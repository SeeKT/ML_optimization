{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 最適化アルゴリズム (1)\n",
    "ここでは，非線形最適化問題を解く代表的なアルゴリズムである最急降下法とニュートン法についてまとめる．[1]を参考にした．\n",
    "\n",
    "[1]: 茨木, [最適化の数学](https://www.kyoritsu-pub.co.jp/kenpon/bookDetail/9784320015654), 共立出版,2011.\n",
    "\n",
    "## 考える問題\n",
    "機械学習では，しばしば以下のような制約なしの最適化問題を解く．\n",
    "\n",
    "$$ \\underset{\\boldsymbol{x}}{\\text{minimize}} \\ \\ f(\\boldsymbol{x}) \\ \\ \\text{subject to} \\ \\ \\boldsymbol{x} \\in S \\subset \\mathbb{R}^N.  \\tag{1}$$\n",
    "\n",
    "ここで，$f: \\mathbb{R^N} \\to \\mathbb{R}$ とし，適当な微分可能性を仮定する．例えば，損失を最小にする重みの学習などがこのような問題で表される．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 解法\n",
    "点 $\\boldsymbol{x}^{*}$ の1次の最適性必要条件は停留点であること，つまり，\n",
    "\n",
    "$$ \\nabla f(\\boldsymbol{x}^{*}) = 0 \\tag{2}$$\n",
    "\n",
    "である．非線形関数 $f$ に対して， (2) を直接解くのは一般に困難である．\n",
    "\n",
    "$\\rightsquigarrow$ 計算を繰り返すことで $\\boldsymbol{x}^{*}$ に収束する点列 $\\{ \\boldsymbol{x}^k \\}$ を生成し， $\\boldsymbol{x}^{*}$ に十分近くなったと判断されたところで計算を終え，その時点の $\\boldsymbol{x}^k$ を $\\boldsymbol{x}^{*}$ の近似解として出力するのが普通．\n",
    "\n",
    "現在の点 $\\boldsymbol{x}^k$ において，ベクトル $d(\\boldsymbol{x}^k) \\in \\mathbb{R}^N$ が\n",
    "\n",
    "$$ \\nabla f(\\boldsymbol{x}^k)^{\\mathrm{T}} d(\\boldsymbol{x}^k) < 0 \\tag{3}$$\n",
    "\n",
    "を満たすならば， $\\nabla f(\\boldsymbol{x}^k)$ ($f(\\boldsymbol{x}^k)$ からの最急増加方向) と $d(\\boldsymbol{x}^k)$ は鈍角をなす．\n",
    "\n",
    "$\\rightsquigarrow$ $d(\\boldsymbol{x}^k)$ は $f$ の降下方向を示す．\n",
    "\n",
    "よって，降下方向ベクトル $d(\\boldsymbol{x}^k)$ を見つけ，\n",
    "\n",
    "$$ \\boldsymbol{x}^{k + 1} = \\boldsymbol{x}^k + \\alpha_k d(\\boldsymbol{x}^k) \\tag{4}$$\n",
    "\n",
    "という修正を行うという方針をとる．$\\alpha_k$ はステップ幅または**学習率**と呼ばれる実数である．\n",
    "\n",
    "$d(\\boldsymbol{x}^k)$ や $\\alpha_k$ をどのように見つけるかによってアルゴリズムが決まる．"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 最急降下法 (Steepest descent / Gradient descent)\n",
    "$d(\\boldsymbol{x}^k)$ として，勾配ベクトル $\\nabla f(\\boldsymbol{x}^k)$ の反対方向\n",
    "\n",
    "$$ d(\\boldsymbol{x}^k) = - \\nabla f(\\boldsymbol{x}^k) \\tag{5}$$\n",
    "\n",
    "を用いるアルゴリズム．1次の方法である．\n",
    "\n",
    "- $d(\\boldsymbol{x}^k)$ は $\\boldsymbol{x}^k$ から見ると最急降下方向．\n",
    "\n",
    "$f(\\boldsymbol{x})$ の関数形によっては更新則 (5) の方向に進んでも関数値が増加することもある．\n",
    "\n",
    "$\\rightsquigarrow$ 学習率 $\\alpha_k$ を\n",
    "\n",
    "$$ \\min \\{f(\\boldsymbol{x}^k + \\alpha d(\\boldsymbol{x}^k)) \\, | \\, \\alpha > 0\\} \\tag{6}$$\n",
    "\n",
    "を実現する $\\alpha$ が存在すればその値に設定する (直線探索)．\n",
    "\n",
    "最急降下法のアルゴリズムを以下に示す．\n",
    "\n",
    "### Algorithm (Steepest)\n",
    "#### Input\n",
    "- $f$: 目的関数\n",
    "- $\\varepsilon$: 閾値\n",
    "#### Output\n",
    "- $\\boldsymbol{x}^{*}$ の近似解 $\\boldsymbol{x}^k$\n",
    "\n",
    "#### 動作\n",
    "1. (初期化) 初期値 $\\boldsymbol{x}^0$ を定める．$k \\leftarrow 0$.\n",
    "1. (終了判定) $\\|\\boldsymbol{x}^{k + 1} - \\boldsymbol{x}^k  \\| < \\varepsilon$ であれば，$\\boldsymbol{x}^k$ を出力して終了．\n",
    "1. (反復) $d(\\boldsymbol{x}^k) \\leftarrow - \\nabla f(\\boldsymbol{x}^k)$. $\\alpha_k \\leftarrow$ 直線探索の近似解とし，(4) で $\\boldsymbol{x}^{k + 1}$ を求める．$k \\leftarrow k + 1$ として 2. へ戻る．\n",
    "\n",
    "### Remark\n",
    "- $\\nabla f$ を解析的に求めるのが困難な場合には自動微分等を用いる．\n",
    "- 最急降下法は，1次近似で最良の方向へ進む．\n",
    "- $\\alpha_k$ が十分に小さいとき，以下の ODE で表される (修正方程式)．\n",
    "    $$ \\dot{\\boldsymbol{x}} = -\\nabla f(\\boldsymbol{x}) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}